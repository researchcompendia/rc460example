library(lda)
setwd("~/Desktop/hlda-c/test2/data")
data = list()
yy = file("titles.dmap", "w")
indi = seq(10,99)
indi_len  = length(indi)
for(i in 1:indi_len){
app = sprintf("analytics/analyst_business_analytics/page_%d.txt",indi[i])
data[[ 5*(i-1) + 1 ]]= readLines(app)
write(app,file=yy)
write("\n")
app = sprintf("analytics/data_mining/page_%d0.txt",indi[i])
data[[ 5*(i-1) + 2 ]]= readLines(app)
write(app,file=yy)
write("\n")
app = sprintf("beauty/hair_loss/page_%d0.txt",indi[i])
data[[ 5*(i-1) + 3 ]]= readLines(app)
write(app,file=yy)
write("\n")
app = sprintf("fitness/body_tone_mini/page_%d0.txt",indi[i])
data[[ 5*(i-1) + 4 ]]= readLines(app)
write(app,file=yy)
write("\n")
app = sprintf("fitness/fat_loss/page_%d0.txt",indi[i])
data[[ 5*(i-1) + 5 ]]= readLines(app)
write(app,file=yy)
write("\n")
}
close(yy)
corpus = lexicalize(data,lower=TRUE)
vocab = corpus$vocab[-1]
vocab = corpus$vocab[word.counts(corpus$documents, corpus$vocab) >= 2]
corpus = lexicalize(data,lower=TRUE, vocab=vocab)
n.doc = length(corpus)
output = list()
n.unique = rep(NA,n.doc)
zz <- file("corpus.dat", "w")
for(i in 1:n.doc){
n.unique[i] = length(word.counts(corpus[[i]]))
output[[i]] = paste(names(word.counts(corpus[[i]])),word.counts(corpus[[i]]),sep=":")
write(c(n.unique[i],output[[i]]),file=zz,ncolumns=n.unique[i]+1)
write("\n")
}
close(zz)
ww <- file("vocab.txt", "w")
write(vocab,file=ww,sep="\n")
close(ww)
library(lda)
setwd("~/Desktop/hlda-c/test2/data")
data = list()
yy = file("titles.dmap", "w")
indi = seq(10,99)
indi_len  = length(indi)
for(i in 1:indi_len){
app = sprintf("analytics/analyst_business_analytics/page_%d.txt",indi[i])
data[[ 5*(i-1) + 1 ]]= readLines(app)
write(app,file=yy)
write("\n")
app = sprintf("analytics/data_mining/page_%d.txt",indi[i])
data[[ 5*(i-1) + 2 ]]= readLines(app)
write(app,file=yy)
write("\n")
app = sprintf("beauty/hair_loss/page_%d.txt",indi[i])
data[[ 5*(i-1) + 3 ]]= readLines(app)
write(app,file=yy)
write("\n")
app = sprintf("fitness/body_tone_mini/page_%d.txt",indi[i])
data[[ 5*(i-1) + 4 ]]= readLines(app)
write(app,file=yy)
write("\n")
app = sprintf("fitness/fat_loss/page_%d.txt",indi[i])
data[[ 5*(i-1) + 5 ]]= readLines(app)
write(app,file=yy)
write("\n")
}
close(yy)
corpus = lexicalize(data,lower=TRUE)
vocab = corpus$vocab[-1]
vocab = corpus$vocab[word.counts(corpus$documents, corpus$vocab) >= 2]
corpus = lexicalize(data,lower=TRUE, vocab=vocab)
n.doc = length(corpus)
output = list()
n.unique = rep(NA,n.doc)
zz <- file("corpus.dat", "w")
for(i in 1:n.doc){
n.unique[i] = length(word.counts(corpus[[i]]))
output[[i]] = paste(names(word.counts(corpus[[i]])),word.counts(corpus[[i]]),sep=":")
write(c(n.unique[i],output[[i]]),file=zz,ncolumns=n.unique[i]+1)
write("\n")
}
close(zz)
ww <- file("vocab.txt", "w")
write(vocab,file=ww,sep="\n")
close(ww)
library(lda)
setwd("~/Desktop/hlda-c/test2/data")
data = list()
yy = file("titles.dmap", "w")
indi = seq(10,99)
indi_len  = length(indi)
for(i in 1:indi_len){
app = sprintf("analytics/analyst_business_analytics/page_%d.txt",indi[i])
data[[ 5*(i-1) + 1 ]]= readLines(app)
write(app,file=yy)
write("\n")
app = sprintf("analytics/data_mining/page_%d.txt",indi[i])
data[[ 5*(i-1) + 2 ]]= readLines(app)
write(app,file=yy)
write("\n")
app = sprintf("beauty/hair_loss/page_%d.txt",indi[i])
data[[ 5*(i-1) + 3 ]]= readLines(app)
write(app,file=yy)
write("\n")
app = sprintf("fitness/body_tone_mini/page_%d.txt",indi[i])
data[[ 5*(i-1) + 4 ]]= readLines(app)
write(app,file=yy)
write("\n")
app = sprintf("fitness/fat_loss/page_%d.txt",indi[i])
data[[ 5*(i-1) + 5 ]]= readLines(app)
write(app,file=yy)
write("\n")
}
close(yy)
corpus = lexicalize(data,lower=TRUE)
vocab = corpus$vocab[-1]
vocab = corpus$vocab[word.counts(corpus$documents, corpus$vocab) >= 2]
corpus = lexicalize(data,lower=TRUE, vocab=vocab)
n.doc = length(corpus)
output = list()
n.unique = rep(NA,n.doc)
zz <- file("corpus.dat", "w")
for(i in 1:n.doc){
n.unique[i] = length(word.counts(corpus[[i]]))
output[[i]] = paste(names(word.counts(corpus[[i]])),word.counts(corpus[[i]]),sep=":")
write(c(n.unique[i],output[[i]]),file=zz,ncolumns=n.unique[i]+1)
write("\n")
}
close(zz)
ww <- file("vocab.txt", "w")
write(vocab,file=ww,sep="\n")
close(ww)
library(lda)
setwd("~/Desktop/hlda-c/test2/data")
data = list()
yy = file("titles.dmap", "w")
indi = seq(10,99)
indi_len  = length(indi)
for(i in 1:indi_len){
app = sprintf("analytics/analyst_business_analytics/page_%d.txt",indi[i])
data[[ 5*(i-1) + 1 ]]= readLines(app)
write(app,file=yy)
write("\n")
app = sprintf("analytics/data_mining/page_%d.txt",indi[i])
data[[ 5*(i-1) + 2 ]]= readLines(app)
write(app,file=yy)
write("\n")
app = sprintf("beauty/hair_loss/page_%d.txt",indi[i])
data[[ 5*(i-1) + 3 ]]= readLines(app)
write(app,file=yy)
write("\n")
app = sprintf("fitness/body_tone_mini/page_%d.txt",indi[i])
data[[ 5*(i-1) + 4 ]]= readLines(app)
write(app,file=yy)
write("\n")
app = sprintf("fitness/fat_loss/page_%d.txt",indi[i])
data[[ 5*(i-1) + 5 ]]= readLines(app)
write(app,file=yy)
write("\n")
}
close(yy)
corpus = lexicalize(data,lower=TRUE)
vocab = corpus$vocab[-1]
vocab = corpus$vocab[word.counts(corpus$documents, corpus$vocab) >= 2]
corpus = lexicalize(data,lower=TRUE, vocab=vocab)
n.doc = length(corpus)
output = list()
n.unique = rep(NA,n.doc)
zz <- file("corpus.dat", "w")
for(i in 1:n.doc){
n.unique[i] = length(word.counts(corpus[[i]]))
output[[i]] = paste(names(word.counts(corpus[[i]])),word.counts(corpus[[i]]),sep=":")
write(c(n.unique[i],output[[i]]),file=zz,ncolumns=n.unique[i]+1)
write("\n")
}
close(zz)
ww <- file("vocab.txt", "w")
write(vocab,file=ww,sep="\n")
close(ww)
library(lda)
setwd("~/Desktop/hlda-c/test2/data")
data = list()
yy = file("titles.dmap", "w")
indi = seq(10,99)
indi_len  = length(indi)
for(i in 1:indi_len){
app = sprintf("analytics/analyst_business_analytics/page_%d.txt",indi[i])
data[[ 5*(i-1) + 1 ]]= readLines(app)
write(app,file=yy)
write("\n")
app = sprintf("analytics/data_mining/page_%d.txt",indi[i])
data[[ 5*(i-1) + 2 ]]= readLines(app)
write(app,file=yy)
write("\n")
app = sprintf("beauty/hair_loss/page_%d.txt",indi[i])
data[[ 5*(i-1) + 3 ]]= readLines(app)
write(app,file=yy)
write("\n")
app = sprintf("fitness/body_tone_mini/page_%d.txt",indi[i])
data[[ 5*(i-1) + 4 ]]= readLines(app)
write(app,file=yy)
write("\n")
app = sprintf("fitness/fat_loss/page_%d.txt",indi[i])
data[[ 5*(i-1) + 5 ]]= readLines(app)
write(app,file=yy)
write("\n")
}
close(yy)
corpus = lexicalize(data,lower=TRUE)
vocab = corpus$vocab[-1]
vocab = corpus$vocab[word.counts(corpus$documents, corpus$vocab) >= 2]
corpus = lexicalize(data,lower=TRUE, vocab=vocab)
n.doc = length(corpus)
output = list()
n.unique = rep(NA,n.doc)
zz <- file("corpus.dat", "w")
for(i in 1:n.doc){
n.unique[i] = length(word.counts(corpus[[i]]))
output[[i]] = paste(names(word.counts(corpus[[i]])),word.counts(corpus[[i]]),sep=":")
write(c(n.unique[i],output[[i]]),file=zz,ncolumns=n.unique[i]+1)
write("\n")
}
close(zz)
ww <- file("vocab.txt", "w")
write(vocab,file=ww,sep="\n")
close(ww)
library(lda)
setwd("~/Desktop/hlda-c/test2/data")
data = list()
yy = file("titles.dmap", "w")
indi = seq(10,99)
indi_len  = length(indi)
for(i in 1:indi_len){
app = sprintf("analytics/analyst_business_analytics/page_%d.txt",indi[i])
data[[ 5*(i-1) + 1 ]]= readLines(app)
write(app,file=yy)
write("\n")
app = sprintf("analytics/data_mining/page_%d.txt",indi[i])
data[[ 5*(i-1) + 2 ]]= readLines(app)
write(app,file=yy)
write("\n")
app = sprintf("beauty/hair_loss/page_%d.txt",indi[i])
data[[ 5*(i-1) + 3 ]]= readLines(app)
write(app,file=yy)
write("\n")
app = sprintf("fitness/body_tone_mini/page_%d.txt",indi[i])
data[[ 5*(i-1) + 4 ]]= readLines(app)
write(app,file=yy)
write("\n")
app = sprintf("fitness/fat_loss/page_%d.txt",indi[i])
data[[ 5*(i-1) + 5 ]]= readLines(app)
write(app,file=yy)
write("\n")
}
close(yy)
corpus = lexicalize(data,lower=TRUE)
vocab = corpus$vocab[-1]
vocab = corpus$vocab[word.counts(corpus$documents, corpus$vocab) >= 2]
corpus = lexicalize(data,lower=TRUE, vocab=vocab)
n.doc = length(corpus)
output = list()
n.unique = rep(NA,n.doc)
zz <- file("corpus.dat", "w")
for(i in 1:n.doc){
n.unique[i] = length(word.counts(corpus[[i]]))
output[[i]] = paste(names(word.counts(corpus[[i]])),word.counts(corpus[[i]]),sep=":")
write(c(n.unique[i],output[[i]]),file=zz,ncolumns=n.unique[i]+1)
write("\n")
}
close(zz)
ww <- file("vocab.txt", "w")
write(vocab,file=ww,sep="\n")
close(ww)
library(lda)
setwd("~/Desktop/hlda-c/test2/data")
data = list()
yy = file("titles.dmap", "w")
indi = seq(10,99)
indi_len  = length(indi)
for(i in 1:indi_len){
app = sprintf("analytics/analyst_business_analytics/page_%d.txt",indi[i])
data[[ 5*(i-1) + 1 ]]= readLines(app)
write(app,file=yy)
write("\n")
app = sprintf("analytics/data_mining/page_%d.txt",indi[i])
data[[ 5*(i-1) + 2 ]]= readLines(app)
write(app,file=yy)
write("\n")
app = sprintf("beauty/hair_loss/page_%d.txt",indi[i])
data[[ 5*(i-1) + 3 ]]= readLines(app)
write(app,file=yy)
write("\n")
app = sprintf("fitness/body_tone_mini/page_%d.txt",indi[i])
data[[ 5*(i-1) + 4 ]]= readLines(app)
write(app,file=yy)
write("\n")
app = sprintf("fitness/fat_loss/page_%d.txt",indi[i])
data[[ 5*(i-1) + 5 ]]= readLines(app)
write(app,file=yy)
write("\n")
}
close(yy)
corpus = lexicalize(data,lower=TRUE)
vocab = corpus$vocab[-1]
vocab = corpus$vocab[word.counts(corpus$documents, corpus$vocab) >= 2]
corpus = lexicalize(data,lower=TRUE, vocab=vocab)
n.doc = length(corpus)
output = list()
n.unique = rep(NA,n.doc)
zz <- file("corpus.dat", "w")
for(i in 1:n.doc){
n.unique[i] = length(word.counts(corpus[[i]]))
output[[i]] = paste(names(word.counts(corpus[[i]])),word.counts(corpus[[i]]),sep=":")
write(c(n.unique[i],output[[i]]),file=zz,ncolumns=n.unique[i]+1)
write("\n")
}
close(zz)
ww <- file("vocab.txt", "w")
write(vocab,file=ww,sep="\n")
close(ww)
library(lda)
setwd("~/Desktop/hlda-c/test2/data")
data = list()
yy = file("titles.dmap", "w")
indi = seq(10,99)
indi_len  = length(indi)
for(i in 1:indi_len){
app = sprintf("analytics/analyst_business_analytics/page_%d.txt",indi[i])
data[[ 5*(i-1) + 1 ]]= readLines(app)
write(app,file=yy)
write("\n")
app = sprintf("analytics/data_mining/page_%d.txt",indi[i])
data[[ 5*(i-1) + 2 ]]= readLines(app)
write(app,file=yy)
write("\n")
app = sprintf("beauty/hair_loss/page_%d.txt",indi[i])
data[[ 5*(i-1) + 3 ]]= readLines(app)
write(app,file=yy)
write("\n")
app = sprintf("fitness/body_tone_mini/page_%d.txt",indi[i])
data[[ 5*(i-1) + 4 ]]= readLines(app)
write(app,file=yy)
write("\n")
app = sprintf("fitness/fat_loss/page_%d.txt",indi[i])
data[[ 5*(i-1) + 5 ]]= readLines(app)
write(app,file=yy)
write("\n")
}
close(yy)
corpus = lexicalize(data,lower=TRUE)
vocab = corpus$vocab[-1]
vocab = corpus$vocab[word.counts(corpus$documents, corpus$vocab) >= 2]
corpus = lexicalize(data,lower=TRUE, vocab=vocab)
n.doc = length(corpus)
output = list()
n.unique = rep(NA,n.doc)
zz <- file("corpus.dat", "w")
for(i in 1:n.doc){
n.unique[i] = length(word.counts(corpus[[i]]))
if (n.unique[i]>0){
output[[i]] = paste(names(word.counts(corpus[[i]])),word.counts(corpus[[i]]),sep=":")
write(c(n.unique[i],output[[i]]),file=zz,ncolumns=n.unique[i]+1)
write("\n")
}
}
close(zz)
ww <- file("vocab.txt", "w")
write(vocab,file=ww,sep="\n")
close(ww)
